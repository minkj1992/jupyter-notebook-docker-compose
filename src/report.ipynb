{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04afa670-7163-4776-b494-078667aaa630",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install grobid-client langchain openai faiss-cpu PyPDF2 tiktoken chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3535cacd-8ebc-4403-9a3d-4ac7bf58eb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, TextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from typing import (\n",
    "    AbstractSet,\n",
    "    Any,\n",
    "    Callable,\n",
    "    Collection,\n",
    "    Dict,\n",
    "    Generator,\n",
    "    List,\n",
    "    Literal,\n",
    "    Mapping,\n",
    "    Optional,\n",
    "    Set,\n",
    "    Tuple,\n",
    "    Union,\n",
    ")\n",
    "\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079347ce-54cb-4fe9-b25f-9fd2485dc318",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def read_kakao(file_path):\n",
    "    file_type = check_export_file_type(file_path)\n",
    "    return parse(file_type, file_path)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe81be47-72c7-49cf-8fa8-cff87da174e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import List, Optional\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders.base import BaseLoader\n",
    "from langchain.document_loaders.helpers import detect_file_encodings\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "import re\n",
    "from datetime import datetime\n",
    "from typing import List, Dict\n",
    "# kakaotalk 메시지 중 날짜표현 패턴\n",
    "# 이를 사용하여 파일이 추출된 소스와 메시지 구분과 \n",
    "kakaotalk_datetime_pattern_dict = {'window_ko_date': \"-{15} [0-9]{4}년 [0-9]{1,2}월 [0-9]{1,2}일 \\S요일 -{15}\",\n",
    "                                'window_ko_time': \"((\\[)([^\\[])+(\\])) ((\\[오)\\S [0-9]{1,2}:[0-9]{1,2}(\\]))\",\n",
    "                                'android_ko': \"([0-9]){4}년 ([0-9]){1,2}월 ([0-9]){1,2}일 오\\S ([0-9]){1,2}:([0-9]){1,2}\",\n",
    "                                'android_en': \"([A-z])+ ([0-9]){1,2}, ([0-9]){4}, ([0-9]){1,2}:([0-9]){1,2} \\SM\",\n",
    "                                    }\n",
    "\n",
    "\n",
    "def _str_to_datetime(file_type, text):\n",
    "    kakaotalk_strptime_pattern_dict = {'ko': '%Y년 %m월 %d일 %p %I:%M',\n",
    "                                        'en': '%B %d, %Y, %I:%M %p',\n",
    "                                        }\n",
    "\n",
    "    language = file_type.split('_')[1]\n",
    "    if language == 'ko':\n",
    "        text = text.replace('오전', 'AM')\n",
    "        text = text.replace('오후', 'PM')\n",
    "\n",
    "    text_dt = datetime.strptime(text, kakaotalk_strptime_pattern_dict[language])\n",
    "    return text_dt\n",
    "\n",
    "\n",
    "def parse(file_type, file_path, encoding, datetime_pattern_dict=kakaotalk_datetime_pattern_dict):\n",
    "    \"\"\"\n",
    "    Parsing the text from a kaotalk_export_file.\n",
    "    This parser divide messages based on datetime_pattern.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_type: string\n",
    "        one of among 'window_ko', 'android_ko' or 'android_en'\n",
    "\n",
    "    file_path: string\n",
    "\n",
    "    datetime_pattern_dict: dict\n",
    "        datetime_pattern used i kaotalk_export_file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    msgs: list\n",
    "        The messages are list of dictionary.\n",
    "        Each dictionary compose of the informtion of each message.\n",
    "        And it has keys, 'datetime,'user_name' and 'text'.\n",
    "    \"\"\"\n",
    "\n",
    "                                        \n",
    "    msgs = []\n",
    "\n",
    "    if file_type == 'window_ko':     # window\n",
    "        date_pattern = datetime_pattern_dict['window_ko_date']\n",
    "        time_pattern = datetime_pattern_dict['window_ko_time']\n",
    "\n",
    "        with open(file_path, encoding=encoding) as file:\n",
    "            # 줄바꿈되어있는 경우도 묶어주기 위해 buffer 사용\n",
    "            buffer = ''\n",
    "            date = ''\n",
    "\n",
    "            for line in file:\n",
    "                # window파일의 데이트str(--------------- 2020년 6월 28일 일요일 ---------------)이거나 시간 str([김한길] [오후 2:15] htt)이면\n",
    "                if re.match(date_pattern, line) or re.match(time_pattern, line):\n",
    "                    # buffer가 time_pattern으로 시작하는 경우만 추가해주기\n",
    "                    if re.match(time_pattern, buffer):  \n",
    "                        buffer_tokens = buffer.split(']', maxsplit=2)\n",
    "                        user_name = buffer_tokens[0].replace('[', '').strip()\n",
    "                        time = buffer_tokens[1].replace('[', '').strip()\n",
    "                        my_datetime = _str_to_datetime(file_type, f\"{date} {time}\")\n",
    "                        text = buffer_tokens[2].strip()\n",
    "                        \n",
    "                        msgs.append({'datetime': my_datetime,\n",
    "                                        'user_name': user_name,\n",
    "                                        'text': text\n",
    "                        })\n",
    "\n",
    "                    if re.match(date_pattern, line):  # window파일의 데이트str이면\n",
    "                        date = line.replace('-', '').strip().rsplit(\" \", 1)[0]\n",
    "                        buffer = ''\n",
    "                    else:  #  window파일의 시간 str이면\n",
    "                        buffer = line\n",
    "\n",
    "                else:\n",
    "                    buffer += line\n",
    "\n",
    "    else: # android\n",
    "        datetime_pattern = datetime_pattern_dict[file_type]\n",
    "        msg_exist_check_pattern = datetime_pattern + \",.*:\"\n",
    "\n",
    "        with open(file_path, encoding=encoding) as file:\n",
    "            # 줄바꿈되어있는 경우도 저장하기 위해 buffer 사용\n",
    "            buffer=''\n",
    "            for line in file:\n",
    "                if re.match(datetime_pattern, line):\n",
    "                    if re.match(msg_exist_check_pattern, buffer):\n",
    "                        \n",
    "                        temp_01_2_tokens = buffer.split(\" : \", maxsplit=1)\n",
    "                        temp_0_1_tokens = temp_01_2_tokens[0].rsplit(\",\", maxsplit=1)\n",
    "\n",
    "                        my_datetime = temp_0_1_tokens[0].strip()\n",
    "                        my_datetime = _str_to_datetime(file_type, my_datetime)\n",
    "                        user_name = temp_0_1_tokens[1].strip()\n",
    "                        text = temp_01_2_tokens[1].strip()\n",
    "                        msgs.append({'datetime': my_datetime,\n",
    "                                    'user_name': user_name,\n",
    "                                    'text': text\n",
    "                        })\n",
    "\n",
    "                    buffer = line\n",
    "                else:\n",
    "                    buffer += line\n",
    "\n",
    "    parsed_msgs = extract(msgs)\n",
    "    return parsed_msgs\n",
    "\n",
    "def extract(msgs: List[Dict[str,str]]):\n",
    "    extracted_msgs = []\n",
    "    exclude_msg_pattern = r'^(이모티콘|사진|동영상|\\.|,|)$'\n",
    "\n",
    "    i = 0\n",
    "    while i < len(msgs):\n",
    "        m = msgs[i]\n",
    "        nick = m.get('user_name')\n",
    "        datetime = m.get('datetime')\n",
    "        text = m.get('text','').strip()\n",
    "\n",
    "        if (nick is None) or (datetime is None) or text == '' or re.match(exclude_msg_pattern, text):\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # If current nick equals to the next nick, combine the texts and continue\n",
    "        if i + 1 < len(msgs) and msgs[i + 1].get('user_name') == nick:\n",
    "            # If the text ends with '\\n', remove it\n",
    "            if text.endswith('\\n'):\n",
    "                text = text[:-1]\n",
    "            \n",
    "            msgs[i + 1]['text'] = text + \" \" + msgs[i + 1]['text'].strip()\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        extracted_msg = f\"{nick} : {text}\"\n",
    "        extracted_msgs.append(extracted_msg)\n",
    "        i += 1\n",
    "\n",
    "    return '\\n'.join(m for m in extracted_msgs)\n",
    "\n",
    "\n",
    "class KakaoTextLoader(BaseLoader):\n",
    "    \"\"\"Load kakao text files.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        file_path: Path to the file to load.\n",
    "\n",
    "        encoding: File encoding to use. If `None`, the file will be loaded\n",
    "        with the default system encoding.\n",
    "\n",
    "        autodetect_encoding: Whether to try to autodetect the file encoding\n",
    "            if the specified encoding fails.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        file_path: str,\n",
    "        encoding: Optional[str] = None,\n",
    "        autodetect_encoding: bool = False,\n",
    "    ):\n",
    "        \"\"\"Initialize with file path.\"\"\"\n",
    "        self.file_path = file_path\n",
    "        self.encoding = encoding\n",
    "        self.autodetect_encoding = autodetect_encoding\n",
    "        self.kakao_file_type = self.check_kakao_export_file_type(file_path)\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def check_kakao_export_file_type(file_path,\n",
    "                                datetime_pattern_dict=kakaotalk_datetime_pattern_dict):\n",
    "        \"\"\"\n",
    "        Check the device type and language of kakaotalk_export_file.\n",
    "        It is done based on datetime patterns in file\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        file_path: string\n",
    "    \n",
    "        datetime_pattern_dict: dict\n",
    "            datetime_pattern used i kaotalk_export_file\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        file_type: string\n",
    "            one of among 'window_ko', 'android_ko' or 'android_en'\n",
    "        \"\"\"\n",
    "    \n",
    "        # 파일의 두 번째 줄(저장한 날짜 : /Date Saved : ) 부분의 날짜형식으로 구분\n",
    "        # kakaotalk_include_date_pattern_dict = {'pc_ko': \"([0-9]){4}-([0-9]){1,2}-([0-9]){1,2} ([0-9]){1,2}:([0-9]){1,2}\",\n",
    "        #                             'mobile_ko': \"([0-9]){4}년 ([0-9]){1,2}월 ([0-9]){1,2}일 오\\S ([0-9]){1,2}:([0-9]){1,2}\",\n",
    "        #                             'mobile_en': \"([A-z])+ ([0-9]){1,2}, ([0-9]){4}, ([0-9]){1,2}:([0-9]){1,2} \\SM\",}\n",
    "        \n",
    "        with open(file_path, 'r') as f:\n",
    "            for counter in range(5):\n",
    "                line = f.readline()\n",
    "                if not line: break\n",
    "    \n",
    "                for file_type, pattern in datetime_pattern_dict.items():\n",
    "                    if re.search(pattern, line):\n",
    "                        \n",
    "                        return '_'.join(file_type.split('_')[:2])\n",
    "        raise ValueError(f'Error: Cannot know the device type and language of the file.\\nPlease check the file is a kakaotalk export file or the export enviroment is in among {str(list(kakaotalk_include_date_pattern_dict.keys()))}')\n",
    "\n",
    "    \n",
    "    def load(self) -> List[Document]:\n",
    "        \"\"\"Load from file path.\"\"\"\n",
    "        text = \"\"\n",
    "        try:\n",
    "            text = parse(self.kakao_file_type, self.file_path, encoding=self.encoding)\n",
    "        except UnicodeDecodeError as e:\n",
    "            if self.autodetect_encoding:\n",
    "                detected_encodings = detect_file_encodings(self.file_path)\n",
    "                for encoding in detected_encodings:\n",
    "                    logger.debug(\"Trying encoding: \", encoding.encoding)\n",
    "                    try:\n",
    "                        with open(self.file_path, encoding=encoding.encoding) as f:\n",
    "                            text = f.read()\n",
    "                        break\n",
    "                    except UnicodeDecodeError:\n",
    "                        continue\n",
    "            else:\n",
    "                raise RuntimeError(f\"Error loading {self.file_path}\") from e\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error loading {self.file_path}\") from e\n",
    "        # TODO: algin with chat datetime in metadata\n",
    "        metadata = {\"source\": self.file_path}\n",
    "        return [Document(page_content=text, metadata=metadata)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a9e08c-0d38-422d-adcb-7e5077bdd63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def split(docs:List[Document], chunk_size):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=0, separators=[\"\\n\\n\", \"\\n\", \" \", \"\"])\n",
    "    return text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e66d822-5db3-47ce-adcb-3896394702ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_token_mapping = {\n",
    "    \"gpt-4\": 8192,\n",
    "    \"gpt-4-0314\": 8192,\n",
    "    \"gpt-4-32k\": 32768,\n",
    "    \"gpt-4-32k-0314\": 32768,\n",
    "    \"gpt-3.5-turbo\": 4096,\n",
    "    \"gpt-3.5-turbo-0301\": 4096,\n",
    "    \"text-ada-001\": 2049,\n",
    "    \"ada\": 2049,\n",
    "    \"text-babbage-001\": 2040,\n",
    "    \"babbage\": 2049,\n",
    "    \"text-curie-001\": 2049,\n",
    "    \"curie\": 2049,\n",
    "    \"davinci\": 2049,\n",
    "    \"text-davinci-003\": 4097,\n",
    "    \"text-davinci-002\": 4097,\n",
    "    \"code-davinci-002\": 8001,\n",
    "    \"code-davinci-001\": 8001,\n",
    "    \"code-cushman-002\": 2048,\n",
    "    \"code-cushman-001\": 2048,\n",
    "}\n",
    "# TODO: prompt number of token\n",
    "def get_batch_size(model_name, k, chunk_size):\n",
    "    return (chunk_size * k) // model_token_mapping[model_name]\n",
    "    \n",
    "    \n",
    "# TODO: max prompt size\n",
    "# InvalidRequestError: This model's maximum context length is 4097 tokens, however you requested 9224 tokens (8968 in your prompt; 256 for the completion). Please reduce your prompt; or completion length.\n",
    "chunk_size = 1500\n",
    "k = 10\n",
    "model_name = \"text-davinci-003\"\n",
    "batch_size = get_batch_size(model_name, k, chunk_size)\n",
    "print(batch_size)\n",
    "\n",
    "\n",
    "loader = KakaoTextLoader('./data/kakao.txt', encoding='utf8')\n",
    "raw_docs = loader.load()\n",
    "test_docs = await split(raw_docs, chunk_size)\n",
    "len(test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21ba429-51ed-418a-85e8-26f4d8dd6af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "db = FAISS.from_documents(docs, embeddings)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f54f04b-ba1e-4a8d-aabf-9f9bf0188218",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.save_local('./data/faiss_kakao')\n",
    "new_db = FAISS.load_local(\"./data/faiss_kakao\", embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d9d328-8026-4daa-934d-489d869c0a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(search_kwargs={'k':k})\n",
    "docs = retriever.get_relevant_documents(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bafe181-a5d9-474f-99e4-acb26ccb93d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default model text-davinci-003\n",
    "llm = OpenAI(batch_size=batch_size, model_name=model_name, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec024428-bfd1-4c93-b058-53a9abbe0412",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def get_summary_map_reduce_prompt(topic):\n",
    "    summary_prompt_template = \"\"\"Summarize the chat conversation that is in the text below, so that the content presented in the topic above is well represented.\n",
    "    You must obtain and summarize the necessary data from text so that the content written in topic can be well represented.\n",
    "    \n",
    "    The CONVERSATION CONTEXT format is 'speaker: message'.    \n",
    "    For example, in 'minwook: my name is minwook', the conversation content is 'my name is minwook'. \n",
    "    The content of the conversation is the most important.\n",
    "    \n",
    "    !IMPORTANT Even if you can't analyze it, guess based on your knowledge. answer unconditionally.\n",
    "    \n",
    "    text: {text}\n",
    "    \n",
    "    \"\"\"\n",
    "    prefix_summary = f\"The topic is '{topic}'.\" \n",
    "    suffix_summary = \"CONCISE SUMMARY IN 3000 WORDS IN ENGLISH:\"\n",
    "    template = prefix_summary + summary_prompt_template + suffix_summary\n",
    "    \n",
    "    return PromptTemplate(template=template, input_variables=[\"text\"])\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "map_reduce_prompt = get_summary_map_reduce_prompt('The major story that happened between Kanghyeon and Soyeon, who are lovers.')\n",
    "chain = load_summarize_chain(\n",
    "        llm=llm, \n",
    "        chain_type=\"map_reduce\",\n",
    "        map_prompt=map_reduce_prompt, \n",
    "        combine_prompt=map_reduce_prompt,\n",
    "        verbose=True,\n",
    ")\n",
    "summary = chain({\"input_documents\": docs}, return_only_outputs=True)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e76c6c-88e2-4da1-b2cd-f841bec782d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# topic='\b연인관계인 강현과 소연이의 둘 사이에 일어났던 주요 사건들과 시간에 따른 서로의 감정상태'\n",
    "def get_summary_refine_prompt(topic='The major \bstory that happened between Kanghyeon and Soyeon, who are lovers.'):\n",
    "    summary_prompt_template = \"\"\"Summarize the chat conversation that is in the text below, so that the content presented in the topic above is well represented.\n",
    "    You must obtain and summarize the necessary data from text so that the content written in topic can be well represented.\n",
    "    \n",
    "    The CONVERSATION CONTEXT format is 'year month day time, speaker: message'.    \n",
    "    For example, in '2000, May 3, 3:00 AM, A: Hello', the conversation content is Hello. \n",
    "    The content of the conversation is the most important.\n",
    "    \n",
    "    !IMPORTANT Even if you can't analyze it, guess based on your knowledge. answer unconditionally.\n",
    "    \n",
    "    text: {text}\n",
    "    \n",
    "    \"\"\"\n",
    "    prefix_summary = f\"The topic is '{topic}'.\" \n",
    "    suffix_summary = \"CONCISE SUMMARY IN 3000 WORDS IN ENGLISH:\"\n",
    "    template = prefix_summary + summary_prompt_template + suffix_summary\n",
    "    \n",
    "    PROMPT = PromptTemplate(template=template, input_variables=[\"text\"])\n",
    "\n",
    "\n",
    "refine_prompt = get_summary_map_reduce_prompt('The major \bstory that happened between Kanghyeon and Soyeon, who are lovers.')\n",
    "chain = load_summarize_chain(\n",
    "        llm=llm, \n",
    "        chain_type='refine',\n",
    "    \n",
    "        verbose=True,\n",
    ")\n",
    "summary = chain({\"input_documents\": docs}, return_only_outputs=True)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f307ff-a6df-4998-9e6c-78e20b0dcf4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
